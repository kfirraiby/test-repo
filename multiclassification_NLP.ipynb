{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multiclassification NLP ",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kfirraiby/test-repo/blob/master/multiclassification_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tbz6JWAAsVmT",
        "colab_type": "text"
      },
      "source": [
        "#ovo multicalssification  -  text sentiment analysis \n",
        "\n",
        "- In this work I will present a classic machine learning model pipeline for  text analysis.\n",
        "\n",
        "- The purpose of the model is to estimate rating that a person will give to a movie based on the review he wrote.\n",
        "\n",
        "- The model is a logistic regression with a lasso penalty, which receives a lemmatized (with part of speech taken into account) n-gram (1,3)  tf-idf as features \n",
        "\n",
        "***note:***:i this file there is a ***full pipeline***  for preprocessing, feature desine, train,test and prediction only for the final modle.\n",
        "there is no  exploratory data analysis or model comparison. all this was done in previous work and this file is the final result  \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O08dZdNQv3FH",
        "colab_type": "code",
        "outputId": "33d22663-b073-4afb-e562-83dd395371a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "!pip install imblearn"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imblearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.6/dist-packages (from imblearn) (0.4.3)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn->imblearn) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn->imblearn) (1.16.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn->imblearn) (0.21.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20->imbalanced-learn->imblearn) (0.13.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LH59She-C2cv",
        "colab_type": "code",
        "outputId": "e5c5f095-65f4-483a-ba55-b0c568dc466e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "import imblearn\n",
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn import  preprocessing\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2XnjDJicxN8",
        "colab_type": "text"
      },
      "source": [
        "#**Reading, uploading and filtering**\n",
        "\n",
        "Because of the computation power, I will filter the data for classes 1 (bad), 3 (ok), and 5 (good).\n",
        "From this I will sample  about 2,000,000 observations while maintaining the imbalanced problem.  \n",
        "<br>\n",
        "The data is raw, meaning that we need to look in the raw text where the Review is and where the ranking is per customer\n",
        "<br>\n",
        "example:\n",
        "<br>\n",
        "<br>\n",
        "product/productId: B00006HAXW\n",
        "<br>\n",
        "review/userId: A1RSDE90N6RSZF\n",
        "<br>\n",
        "review/profileName: Joseph M. Kotow\n",
        "<br>\n",
        "review/helpfulness: 9/9\n",
        "<br>\n",
        "review/score: 5.0\n",
        "<br>\n",
        "review/time: 1042502400\n",
        "<br>\n",
        "review/summary: Pittsburgh - Home of the OLDIES\n",
        "<br>\n",
        "review/text: I have all of the doo wop DVD's and this one is as good or better than the\n",
        "1st ones. Remember once these performers are gone, we'll never get to see them again.\n",
        "Rhino did an excellent job and if you like or love doo wop and Rock n Roll you'll LOVE\n",
        "this DVD !!\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "you can find the details on the raw data in https://snap.stanford.edu/data/web-Movies.html\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkFxi3MCgfYQ",
        "colab_type": "code",
        "outputId": "ec823bb7-2b91-4382-c6e2-121a65458d33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Connecting to Google Drive where the raw data is\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6Hm9qMODHT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Search and save the ranking from raw text\n",
        "file = open(\"/content/drive/My Drive/Colab Notebooks/movies.txt\", \"r\",errors=\"ignore\")\n",
        "scores = []\n",
        "for line in file: \n",
        "  if line[:13] == \"review/score:\":\n",
        "    scores.append(float(line[14:]))\n",
        "\n",
        "#Search and save the review\n",
        "file =  open(\"/content/drive/My Drive/Colab Notebooks/movies.txt\", \"r\",errors=\"ignore\") \n",
        "reviews = []\n",
        "for line in file: \n",
        "  if line[:12] == \"review/text:\":\n",
        "    reviews.append(line[12:])\n",
        "    \n",
        "file.close()\n",
        "\n",
        "#saving in a pandas DataFrame\n",
        "df = pd.DataFrame({'score': scores ,'text': reviews})\n",
        "del scores\n",
        "del reviews\n",
        "\n",
        "# sampling and maintaining the imbalanced problem.  \n",
        "data_full = df.sample(n = 2000000, weights=\"score\",replace=False,random_state=42)[df[\"score\"].isin([1,3,5])]\n",
        "del df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hy2y3NVz-lGT",
        "colab_type": "code",
        "outputId": "656a39ee-1aac-438b-9e37-0eddc16485cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "print(data_full.score.value_counts(normalize = True))\n",
        "print(data_full.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5.0    0.870197\n",
            "3.0    0.100803\n",
            "1.0    0.028999\n",
            "Name: score, dtype: float64\n",
            "(1523559, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxCsm9Xv6cDT",
        "colab_type": "code",
        "outputId": "f78d99f4-0efd-42bc-f8a8-901874e66e2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "#example\n",
        "print(data_full[\"text\"].loc[2968573])\n",
        "print(data_full[\"score\"].loc[2968573])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " In a true sense, Clint Eastwood & writer David Peoples have created the first Anti-western in this tale of violence, vengeance and half-truths. Eastwood plays the anti-hero William Munny, once a vicious killer, now reformed eking out an existence for his two motherless children by pig farming. To save his children from this hapless fate he agrees to go on one last mercenary killing to avenge a prostitute whose face, eyes, ears and teets were cut up by some clients or so he's heard. But the truth of it is less than the fiction as the whore's story, like so many tales from the old west, is continually embellished. Urged on by the raging Schofield Kid, a haplessly poor-sighted lad who only dreams of the notoriety his killing might bring, Munny enlists the aid of Morgan Freeman, his old partner. Munny so rusty he can no longer aim well and so long out of the saddle he can't even stay on his horse, he heads out with the other two avenging or fallen angels to Big Whiskey to hunt down the killers for the promise of the whore's gold. Although some might consider the character insignificant, Richard Harris, as English Bob, the Duke (Duck) of Death, further illustrates the fallacy of the old west allure. Gene Hackman as the sadistic Little Bill Daggett, the sheriff of Big Whiskey, further blurs the lines between good and evil as the townspeople wince from the pain he inflicts on English Bob, Munny and finally on Ned Logan (Freeman). Frances Fisher, a lone voice crying in the wilderness, demands justice for the unfortunate girl and all women despite their station in life. Saul Rubinek as an old west biographer, is reminiscent of those mesmerized by car wrecks or burning buildings. A cub-reporter, only concerned for the most engrossing story, he hops from supposed hero to supposed hero never once looking beneath the surface of their stories. In his review, Leonard Maltin feels the movie drags in the middle, but the slow paced path the trio takes amidst the beautiful countryside, like the gentle but haunting score, only serves as the antithesis to the movie's impending violent conclusion. When I first saw this movie at the theater, I whispered to my companion \"this is going to win best picture.\" It also went on to win Eastwood best director and Hackman best supporting actor. Whether you love westerns or hate them this film will satisfy you. There's no glamour in killing, but Munny (Eastwood), whether fueled by his sense of justice or just whiskey, marshals up the malicious character buried deep within him and perhaps buried deep within all of us. Finally, as this morality play draws to a close, we are asked the question why Claudia, Munny's deceased wife a woman not without prospects, ever \"loved\" such a desperate character. Perhaps he, like all the characters, had it coming. Just like we all do, even if we are unforgiven.<br /><br />D Wood & J Mark see this as some allegory for America or banter that it's liberals playing conservatives.  They need to grow up just like the Western did.  For years Hollywood portrayed the Western as a pristeen fantasy place with white hats chasing after the black hats, but it's just not that clear cut except in their own little minds emphasis on little.  One moral they seem to have missed from Unforgiven is that although killing is a terrible thing to do, sometimes it's the only thing that you must do, but it's not the ONLY thing you should do.  I am sure these idiots would bring popcorn and raisinettes to an execution because they're just those types of people.  And those who find it slow (and you know who you are), you're probably under the age of 10 or at least mentally under 10 and can't see that it's more exciting to be lead down a prim rose path and then shockingly be pelted with individual or random acts of violence instead of your typical slasher movie with a spewing of blood every minute.  If you didn't wince from what Sheriff Bill Dagget did to the Duck or William Munny, you've never experienced pain.  Now don't get me wrong, I love the Lone Wolf & Cub series with its spew after spew of blood, but you really can't take them too seriously.  But if you want mindless blood spurting entertainment re-rent ALL the Friday the 13th or the Nightmare series -- leave the serious films to the Adults.\n",
            "\n",
            "5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pd4rbn7pgBQD",
        "colab_type": "text"
      },
      "source": [
        "#**text preration class**\n",
        "\n",
        "-   ***costume stop words*** \n",
        "-   ***lower case and remove special characters, numbers, punctuation and others***\n",
        "-  ***lemmilemmatize by part of speech***\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5_98YZUaJ7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# extract POS tags in dict\n",
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].lower()\n",
        "    tag_dict = {\"j\": wordnet.ADJ,\n",
        "                \"n\": wordnet.NOUN,\n",
        "                \"v\": wordnet.VERB,\n",
        "                \"r\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "# i implemnt the text preperation in a class that we can use it in a pipeline later on \n",
        "\n",
        "class TextPrepTransformer(BaseEstimator, TransformerMixin):\n",
        "  def fit(self, X,y=None):\n",
        "    return self\n",
        "  def transform(self,X):\n",
        "      remove = [\"don\",\"don't\",\"no\",\"not\",\"too\",\"very\",\"isn\",\"isn't\",\"wasn\",\"wasn't\",\"weren\",\"weren't\",\"won\",\"won't\",\"wouldn\",\"wouldn't\",\"shouldn't\",\"shouldn\",\"mustn't\",\"mustn\"]\n",
        "      S_words = list(stopwords.words(\"english\"))\n",
        "      word_tokenizer = WhitespaceTokenizer()\n",
        "      lemmatizer = WordNetLemmatizer()\n",
        "#remove costume stopwords\n",
        "      new_list = []\n",
        "      for e in S_words:\n",
        "          if e not in remove:\n",
        "              new_list.append(e)\n",
        "      new_list.append(\"www\")\n",
        "      new_list.append(\"com\")\n",
        "      new_list.append(\"quot\")\n",
        "      new_list.append(\"br\")\n",
        "      S_words = new_list\n",
        "\n",
        "  #clean text\n",
        "      X = X.apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "      X = X.apply(lambda x: \" \".join(x.strip() for x in x.split()))\n",
        "      X = X.replace(r\"http\\S+\", \"\")\n",
        "      X = X.replace(r\"www.\\S+\", \"\")\n",
        "      X = X.str.replace('[^A-Za-z]+', ' ')\n",
        "      X = X.str.replace(r\"[^\\w\\s]\", '')\n",
        "      X = X.apply(lambda x: ' '.join([word for word in x.split() if word not in (S_words)]))\n",
        "      X = X.astype(str)\n",
        "    \n",
        "  #lemmitize by part of speech and tokenize\n",
        "      for i in X.index:\n",
        "        X[i] = [lemmatizer.lemmatize(w,get_wordnet_pos(w)) for w in word_tokenizer.tokenize(X[i])]\n",
        "  \n",
        "      return X\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "   \n",
        "    \n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-uG0hqdz75x",
        "colab_type": "text"
      },
      "source": [
        "#***imbalance problem*** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l85CWJHblLUS",
        "colab_type": "text"
      },
      "source": [
        "For the problem of imbalance I wanted to use more complex techniques.\n",
        "For each model I wanted to use an undersampling method that takes into account the density.\n",
        "<br>\n",
        "note that oversampling here will create a lot of noisy data because  of high sparsity and  is not necessary sience we have  fair number of observations in the small class\n",
        "\n",
        "when i tested the undersampling method that takes into account the density (NearMiss algo) it was ***after*** processing the data and extracting the features, but **unfortunately**, there was not enough computing power.\n",
        " So I just had to undesampled it randomly before processing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hNvXzcfWSQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RS = RandomUnderSampler(\"not minority\")\n",
        "X_sm, y_sm = RS.fit_sample(pd.DataFrame(data_full[\"text\"]),data_full[\"score\"].astype(\"category\"))\n",
        "data_full = pd.merge(pd.DataFrame(X_sm),pd.DataFrame(y_sm),left_index=True,right_index=True)\n",
        "data_full = data_full.rename(columns={\"0_x\": \"text\", \"0_y\": \"score\"})\n",
        "del X_sm, y_sm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_bVbYIwXTfq",
        "colab_type": "code",
        "outputId": "7ec863ea-1e2d-4c30-f59d-744d84087458",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "data_full.score.value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.0    44182\n",
              "3.0    44182\n",
              "1.0    44182\n",
              "Name: score, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpMzlfSLe5YK",
        "colab_type": "text"
      },
      "source": [
        "# train and test in a pipeline\n",
        "\n",
        "This model is the chosen model after hyperparameter tuning, comparing different algorithms and comparing the different combinations of the NLP techniques above\n",
        "\n",
        "tested:\n",
        "- naive baye, logistic regression and  random forest \n",
        "-  stemming, lemmatization and lemmatization + POS\n",
        "- countvectorizer, 1-3 gram, tf-idf (and combined)\n",
        "- random under sampling and near miss\n",
        "\n",
        "all of the above where tested with a simple train test split and chosen by acuuracy and auc score\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mjrSSzhg2AA",
        "colab_type": "code",
        "outputId": "bbc4947a-d604-47d1-dd66-03a0ddc3f5e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        }
      },
      "source": [
        "\n",
        "models =  [\"53\",\"51\",\"31\"]\n",
        "\n",
        "for idx, mod in enumerate(models):\n",
        "  #filter data for the model\n",
        "  data = data_full.loc[(data_full[\"score\"]==float(mod[0])) | (data_full[\"score\"]==float(mod[1]))]\n",
        "  print(\"model:\", mod)\n",
        " \n",
        "  X_model, X_test_final, y_model, y_test_final = train_test_split(data[\"text\"], data[\"score\"], test_size = 0.1, random_state=42,shuffle = True)\n",
        "  \n",
        "  if mod == \"53\":\n",
        "    \n",
        "    model_53 = Pipeline(steps = [ \n",
        "                           (\"text prep\",TextPrepTransformer()),\n",
        "                           (\"cvec\",CountVectorizer(min_df = 0.005,max_df=0.9,ngram_range=(1,3), tokenizer=lambda doc: doc, lowercase=False)),\n",
        "                           (\"TD\", TfidfTransformer()),\n",
        "                           (\"estimator53\", LogisticRegression(penalty=\"l1\"))\n",
        "                          ])\n",
        "    model_53.fit(X_model,y_model)\n",
        "    y_pred_test = model_53.predict(X_test_final)\n",
        "    print(\"test results:\",classification_report(y_test_final,y_pred_test))\n",
        "  \n",
        "  elif mod == \"51\":\n",
        "    model_51 = Pipeline(steps = [ \n",
        "                           (\"text prep\",TextPrepTransformer()),\n",
        "                           (\"cvec\",CountVectorizer(min_df = 0.005,max_df=0.9,ngram_range=(1,3), tokenizer=lambda doc: doc, lowercase=False)),\n",
        "                           (\"TD\", TfidfTransformer()),\n",
        "                           (\"estimator51\", LogisticRegression(penalty=\"l1\"))\n",
        "                            ])\n",
        "    model_51.fit(X_model,y_model)\n",
        "    y_pred_test = model_51.predict(X_test_final)\n",
        "    print(\"test results:\",classification_report(y_test_final,y_pred_test))\n",
        "    \n",
        "  elif mod == \"31\":\n",
        "    model_31 = Pipeline(steps = [ \n",
        "                           (\"text prep\",TextPrepTransformer()),\n",
        "                           (\"cvec\",CountVectorizer(min_df = 0.005,max_df=0.9,ngram_range=(1,3), tokenizer=lambda doc: doc, lowercase=False)),\n",
        "                           (\"TD\", TfidfTransformer()),\n",
        "                           (\"estimator31\", LogisticRegression(penalty=\"l1\"))\n",
        "                          ])\n",
        "    model_31.fit(X_model,y_model)\n",
        "    y_pred_test = model_31.predict(X_test_final)\n",
        "    print(\"test results:\",classification_report(y_test_final,y_pred_test))\n",
        "  \n",
        "\n",
        "    \n",
        "  \n",
        " \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model: 53\n",
            "test results:               precision    recall  f1-score   support\n",
            "\n",
            "         3.0       0.83      0.83      0.83      4470\n",
            "         5.0       0.83      0.83      0.83      4367\n",
            "\n",
            "    accuracy                           0.83      8837\n",
            "   macro avg       0.83      0.83      0.83      8837\n",
            "weighted avg       0.83      0.83      0.83      8837\n",
            "\n",
            "model: 51\n",
            "test results:               precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.92      0.91      0.92      4470\n",
            "         5.0       0.91      0.92      0.92      4367\n",
            "\n",
            "    accuracy                           0.92      8837\n",
            "   macro avg       0.92      0.92      0.92      8837\n",
            "weighted avg       0.92      0.92      0.92      8837\n",
            "\n",
            "model: 31\n",
            "test results:               precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.84      0.83      0.84      4470\n",
            "         3.0       0.83      0.84      0.84      4367\n",
            "\n",
            "    accuracy                           0.84      8837\n",
            "   macro avg       0.84      0.84      0.84      8837\n",
            "weighted avg       0.84      0.84      0.84      8837\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQD8SzPEhLg9",
        "colab_type": "text"
      },
      "source": [
        "#***prediction***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Umg3jvZhwJ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#majority vote\n",
        "\n",
        "def vote(text):\n",
        "  result = []\n",
        "  for mod in [model_53,model_51,model_31]:\n",
        "    result.append(float(mod.predict(pd.Series(text))))\n",
        "  d = Counter(result)\n",
        "  inverse = [(value, key) for key, value in d.items()]    \n",
        "  return print(\"review:\",text,\"\\n\",\"estimated score:\" ,max(inverse)[1],\"\\n\",\"--------------------\")\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHriZDZ0tBCe",
        "colab_type": "code",
        "outputId": "236eee9f-c5b1-4021-b24d-393c6c2cd1a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "source": [
        "# prediction\n",
        "vote(\"i love that movie\")\n",
        "vote(\"i hate this movie\")\n",
        "vote(\"the movie was nice, not more then that\")\n",
        "vote(\"i love steven spielberg but this was terrible\")\n",
        "vote(\"I realy  looked forward to this film and it was exactly what I expected\")\n",
        "vote(\"I realy looked forward to this movie and i was a bit disappointed\")\n",
        "vote(\"i hate the actor but it was amazing\")\n",
        "vote(\"i hate the actor and the movie\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "review: i love that movie \n",
            " estimated score: 5.0 \n",
            " --------------------\n",
            "review: i hate this movie \n",
            " estimated score: 1.0 \n",
            " --------------------\n",
            "review: the movie was nice, not more then that \n",
            " estimated score: 3.0 \n",
            " --------------------\n",
            "review: i love steven spielberg but this was terrible \n",
            " estimated score: 1.0 \n",
            " --------------------\n",
            "review: I realy  looked forward to this film and it was exactly what I expected \n",
            " estimated score: 5.0 \n",
            " --------------------\n",
            "review: I realy looked forward to this movie and i was a bit disappointed \n",
            " estimated score: 3.0 \n",
            " --------------------\n",
            "review: i hate the actor but it was amazing \n",
            " estimated score: 5.0 \n",
            " --------------------\n",
            "review: i hate the actor and the movie \n",
            " estimated score: 1.0 \n",
            " --------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}